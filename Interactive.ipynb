{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b062a7f7-32d5-4f99-a09a-b4a466348c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22284a76-963a-47d9-855b-4cfa0ec3c433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AWS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "config.sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6741d35f-d1ed-46e1-b13a-e9a3b6d09022",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e24442a-c432-49ee-bb49-1182e91d5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.1.2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e80b3ece-9460-4b15-a022-e913fda0f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-dend/song_data/A/B/C/*.json\n"
     ]
    }
   ],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"/home/sam/Documents/Udacity Data Engineer/Project 4 - Data Lake/Udacity---Data-Lake-Project/Output/\"\n",
    "\n",
    "# get filepath to song data file\n",
    "song_data = input_data + \"song_data/A/B/C/*.json\"\n",
    "print(song_data)\n",
    "# read song data file\n",
    "df = spark.read.json(song_data).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294258b-3fe5-48dd-8ce6-e01cbf61289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = df.select(\"song_id\",\n",
    "                        \"title\",\n",
    "                        \"artist_id\",\n",
    "                        \"year\",\n",
    "                        \"duration\"\n",
    "                        ).drop_duplicates()\n",
    "    \n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.parquet(output_data + \"songs_table/\",\n",
    "                            mode=\"overwrite\",\n",
    "                            partitionBy=[\"year\", \"artist_id\"]\n",
    "                            )\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.select(\"artist_id\",\n",
    "                            \"artist_name\",\n",
    "                            \"artist_location\",\n",
    "                            \"artist_latitude\",\n",
    "                            \"artist_longitude\"\n",
    "                            ).drop_duplicates()\n",
    "    \n",
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(output_data + \"artists_table/\",\n",
    "                                mode=\"overwrite\"\n",
    "                                )\n",
    "\n",
    "# get filepath to log data file\n",
    "log_data = input_data + \"log_data/*.json\"\n",
    "\n",
    "# read log data file\n",
    "log_df = spark.read.json(log_data).drop_duplicates()\n",
    "    \n",
    "# filter by actions for song plays\n",
    "log_df = log_df.filter(log_df.page == \"NextSong\")\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = log_df.select(\"userId\",\n",
    "                                \"firstName\",\n",
    "                                \"lastName\",\n",
    "                                \"gender\",\n",
    "                                \"level\",\n",
    "                                ).drop_duplicates()\n",
    "    \n",
    "# write users table to parquet files\n",
    "users_table.write.parquet(output_data + \"users_table/\",\n",
    "                              mode=\"overwrite\"\n",
    "                              )\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: x / 1000, TimestampType())\n",
    "log_df = log_df.withColumn(\"time_stamp\", get_timestamp(\"ts\"))\n",
    "    \n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x), TimestampType())\n",
    "log_df = log_df.withColumn(\"date_time\", get_datetime(log_df.time_stamp))\n",
    "    \n",
    "# extract columns to create time table\n",
    "time_table = log_df.withColumn(\"hour\", hour(\"date_time\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"date_time\")) \\\n",
    "        .withColumn(\"week\", weekofyear(\"date_time\")) \\\n",
    "        .withColumn(\"month\", month(\"date_time\")) \\\n",
    "        .withColumn(\"year\", year(\"date_time\")) \\\n",
    "        .withColumn(\"weekday\", dayofweek(\"date_time\")) \\\n",
    "        .select(\"ts\", \"date_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()\n",
    "    \n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.parquet(output_data + \"time_table/\",\n",
    "                             mode='overwrite',\n",
    "                             partitionBy=[\"year\", \"month\"]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d18033-11bf-46e4-9f6c-39c3adca80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.option(\"basePath\", output_data + \"songs_table/\").format(\"parquet\").load(output_data + \"songs_table/*/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044ac26-7746-46d0-9412-6fe4ccd05ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9df10f-a468-4449-9ede-903c8e4b4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = log_df.join(song_df, log_df.song == song_df.title, how='inner').select(monotonically_increasing_id().alias(\"songplay_id\"),\n",
    "                col(\"date_time\"),\n",
    "                col(\"userId\").alias(\"user_id\"),\n",
    "                col(\"level\"),\n",
    "                col(\"song_id\"),\n",
    "                col(\"artist_id\"),\n",
    "                col(\"sessionId\").alias(\"session_id\"),\n",
    "                col(\"location\"),\n",
    "                col(\"userAgent\").alias(\"user_agent\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1f347-0ab3-490d-9463-97c8421fc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table.join(time_table, songplays_table.date_time == time_table.date_time,\n",
    "                                           how=\"inner\") \\\n",
    "        .select(col(\"songplay_id\"),\n",
    "                songplays_table.date_time,\n",
    "                col(\"user_id\"),\n",
    "                col(\"level\"),\n",
    "                col(\"song_id\"),\n",
    "                col(\"artist_id\"),\n",
    "                col(\"session_id\"),\n",
    "                col(\"location\"),\n",
    "                col(\"user_agent\"),\n",
    "                col(\"year\"),\n",
    "                col(\"month\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3567042-33c7-426c-b8a0-d6c9f3f6db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd071eb5-210d-4efe-be83-c3efc19c944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.drop_duplicates().write.parquet((output_data + \"songplays_table/\"),\n",
    "                                                    mode=\"overwrite\",\n",
    "                                                    partitionBy=[\"year\",\"month\"]\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e445fb0-21fd-4c19-b881-974391da045b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
